---
title: Building a data pipeline with R & managing with Apache Airflow
author: Mark Druffel
date: '2020-01-22'
slug: r_and_airflow
categories: []
tags:
  - workflow_management
  - python
  - ubuntu
  - airflow
  - RScript
  
images: ~
output:
  blogdown::html_page:
    toc: true
---

```{r setup, cache = F, include=F, echo=F}
library(knitr)
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, include=FALSE, fig.align="center")
```
## Background
Have you ever written a script or built an Excel workbook to process data for a colleague to then run the same process over and over when they ask? Maybe that same script or workbook often had additional features tacked on over time until you owned a brittle, monolithic process that was a thorn in your side...  I've found myself in that predicment  a number of times. Learning to code reduced most of the pain, but there's still the challenge of scheduling and managing code as tasks without finding yourself in a full-time operational role. I've spent the past few months trying to learn more solutions to this problem and I want to reflect back on the journey and share a solution that's working pretty well for me. 

Data pipelines and workflow management are topics I think a lot of people find confusing so, before I get to a solution, I want to discuss the what, the why, and the how. If you're just here for code, jump to the [Building a Pipeline](#building-a-pipeline) section. :point_down:

### What is a data pipeline  
I get the question a lot, from technical and non-technical  people alike so I'll [David Robinson's](https://twitter.com/drob/status/928447584712253440) advice and get my answer in a blog post... According to Wikipedia, a [data pipeline](https://en.wikipedia.org/wiki/Pipeline_(computing)) is "a set of data processing elements connected in series, where the output of one element is the input of the next one." This definition is simple, but general. Many times the follow-up question is, "what's the difference between a data pipeline and [ETL](https://en.wikipedia.org/wiki/Extract%2C_transform%2C_load)?" Jake Stein wrote an answer to that question in [this](https://www.quora.com/What-is-the-difference-between-a-data-pipeline-and-an-ETL-pipeline?share=1) Quora post. He more or less states that data pipelines are a general pattern and ETL implements that pattern in a specific way. So in plain English, a data pipeline is a series of connected steps that input data and output data. Data pipelines are a core component of an analytics organization's infrastructure and enable the organization to [vertically scale](https://stackoverflow.com/questions/11707879/difference-between-scaling-horizontally-and-vertically-for-databases#11715598) these steps more effectively (from an HR & IT perspective). 

### Why use a data pipeline  
As a data scientist, I use my analytics toolkit to answer questions using data. The "using data" part is the elephant in the room. The majority of corporate data never makes it to a location that analytics professionals can access. I've owned business processes that were bootstrapped by Excel workbooks that would make my computer's fan fire up as soon as my mouse hovered over icon... those Excel workbooks were packed with key business data that only my team had access to, which greatly limits the value. Building data pipelines is an approach to get data from business applications and business process to data consumers and to get analytics from the analytics functions back to the business applications and business processes. For [tidyverse](https://www.tidyverse.org/) users, we use similarly use `%>%` to chain together data operations. 

I would argue pretty much any functioning business has data pipelines, but many of them aren't using a technical approach to vertically scale. That's, in part, why many corporations end up with massive operations organizations that hold processes together with Microsoft Office and brute force. 

### How a data pipeline works  
Let's demonstrate how this all works with a theoretical example. I grew up in a family business, landscaping. The company sells work (contracts) to clients the same way any services firm would. That contract is a data asset containing key attributes about the project including pricing, estimated materials, estimated labor, etc.

![](/posts/images/r_and_airflow/contract_icon.png){width=10% height=10%}

The landscaping crews will document records for each contract executed including a materials sheet (e.g. plants used, soil used, mulch used, equipment used, etc.). The materials sheet also contains key attributes (e.g. actual expenses) of each contract. 

![](/posts/images/r_and_airflow/tree_icon.png){width=15% height=15%}

The crew lead records hours worked for each resource on the contract in an hours sheet. Exactly like the materials sheet, the hours sheet contains key attributes of each contract. 

![](/posts/images/r_and_airflow/clock_icon.png){width=10% height=10%}

At the end of each week the back-office processes payroll and distributes checks. The hours sheet is an input to payroll since many resources are hourly employees. Payroll is also a data asset which is critical to calculating margins for each contract, but it does not explicitly contain key attributes of the contract because it's processed on a weekly basis, not a contract basis. 

![](/posts/images/r_and_airflow/payroll_icon.png){width=15% height=15%}

When a contract is complete, the customer is invoiced. The invoice contains key attributes (e.g. actual income) of each contract. 

![](/posts/images/r_and_airflow/invoice_icon.png){width=10% height=10%}

Once the contract is closed the company will follow up to assess the satisfaction of the client with a net promoter survey. 

![](/posts/images/r_and_airflow/thumbs_up_icon.png){width=10% height=10%}

So that was a lot of words and pictures to say my family's landscaping company runs like a business... They record data related to events, agreements, etc. in order to execute processes that support their teams who are out beautifying clients' properties. In order to do so they often have to bring multiple data sources together to execute a process or inform a decision. 

The team may need to decrease costs, but want to be certain that they don't impact quality. To determine what costs to cut, they may want to review all contract costs over time in order to better understand where the major costs are incurred. To facilitate, someone would pull together data from the supply sheets, the hours sheets, and the contracts (fixed costs), likely by hand in Excel. Doing so is cumbersome and error prone, but it gets the job done and the team is able to get value from the information. 

![](/posts/images/r_and_airflow/excel_process.png){width=50% height=50%}

The team reviews the cost data over time and starts asking more intelligent questions. They realize the cost data alone is not adequate context to make a decision. They need to see costs in the context of margins. To facilitate that need, they add payroll data and invoice data to the contract cost data to make another transformation layer. Further, in order to account for quality they decide to analyze the impact of certain project costs on NPS. If you reflect back on our definition of a data pipeline, they have now built one in Excel. Unfortunately the burden and risk of managing that process in Excel is high...

![](/posts/images/r_and_airflow/excel_process_cont.gif){width=50% height=50%}

In order to reduce the cost and risks the company hires a data analyst writes scripts to process the data transformations automagically. Voil√†, problem solved :clap: Now the company can continue to progress without needing to hire an army of back-office resources. 

![](/posts/images/r_and_airflow/R_process.png){width=50% height=50%}

As the pipeline grows, additional processes will get added to the pipeline over time and eventually running the transformations manually becomes a burden. The cost of running these transformations manually is nominal, but it's not stimulating work for anyone and users tend to be impatient if they rely on that data for their own work. That's where you'll want to use a task scheduler. If you're in Unix/Linux cron is the easy first step and if you're in Windows there's a program called Task Scheduler. There's also an R package for each called [cronR](https://github.com/bnosac/cronR) and [taskscheduleR](https://github.com/bnosac/taskscheduleR) respectively. Now we can schedule our transformations as a task and it will run without us.

![](/posts/images/r_and_airflow/r_process_scheduled.png){width=50% height=50%}

Now the pipeline runs every morning and our users have what they need without the operational burden. However, if one of the transformations fails the entire pipeline fails because it's one body of code. We can break the code up into chunks and manage them as separate tasks to avoid this issue. If one transformation fails the others will still execute with stale data from that task. That's not ideal, but manageable for this tiny pipeline.

![](/posts/images/r_and_airflow/r_process_scheduled_fails.gif){width=50% height=50%}

If the company expanded operations across multiple states and had to manage multiple locations with thousands of contracts a year, we wouldn't be able to manage these upstream job failures without signifigant operational costs. Determining which processes failed and what was impacted is tedious work. Meanwhile, users will continue making decisions based off the stale data which leads to trust issues in the long-term. With only 3 locations it looks, and could even be, manageable. But with 30 or 300 it becomes completely untenable. 

![](/posts/images/r_and_airflow/r_process_scheduled_multipleLocals.png){width=120% height=120%}

As I mentioned in the intro, I have hit this wall a number of times and this is where workflow management comes in. Workflow management tools can schedule tasks, but many are also capable of managing and monitoring tasks with intelligence. In the case of our multiple location example, we can set each task to start when it's parent task(s) complete. These tools use [DAGS](https://www.astronomer.io/blog/what-exactly-is-a-dag/), which are basically the graphs we build when we form our sequences of tasks, to determine job dependencies at runtime.  

There are too many workflow management tools to count, but I found this [list](https://github.com/common-workflow-language/common-workflow-language/wiki/Existing-Workflow-systems) very helpful to narrow my search a bit. A not to R users, [drake](https://github.com/ropensci/drake) can help manage the task dependencies and is an excellent solution. However, many of my projects require a scheduler that can manage multiple languages so it doesn't always suit my needs.  

I had some exposure to some of the more mature solutions including [Talend](https://www.talend.com), Alteryx (https://www.alteryx.com), [Apache-Airflow](https://airflow.apache.org/), and [Luigi](https://github.com/spotify/luigi) over the years. I also found some newer options like [Metaflow](https://metaflow.org/) and [Prefect](https://www.prefect.io/). After doing a fair amount of research and taking a few of them for test drives I decided to go with Airflow for now. I landed on Airflow because it's open source (nonnegotiable for me), it has a large community, the learning curve seemed fairly low, and it seems really flexible. There are some complexities in managing Airflow a few of which are eloquently explained in this post by [Jessica Laughlin](https://medium.com/bluecore-engineering/were-all-using-airflow-wrong-and-how-to-fix-it-a56f14cb0753), as well as some potential design flaws that [Chistopher White](https://medium.com/the-prefect-blog/why-not-airflow-4cfa423299c4) outlined (disclaimer, Christopher White is the CTO @ Prefect). That being said, I'm really just looking for a smarter cron (at least for now) so Airflow seems like it should work well for me. 

## Building a Pipeline  
once I selected a solution to use, I had to set it up. I set up Airflow on [EC2](https://aws.amazon.com/ec2/) with a Postgres backend. I'll walk through the process by writing a few simple R functions to source and transform data, store the data in a data lake ([AWS S3](https://aws.amazon.com/s3/)), schedule those functions as tasks, and review additional features in Airflow that help me manage those tasks.  

### What is a data lake? 
For those who aren't familiar, a [data lake](https://en.wikipedia.org/wiki/Data_lake) "is a system or repository of data stored in its natural/raw format, usually object blobs or files." Basically, a folder on your operating system like *My Documents* for Windows users. Folders and meta data are the structure and structure is not enforced (i.e. if you try to save your homework in the folder labeled "Fun" your computer won't throw an error). Many companies use data pipelines with a data lake as the first (or more) layer of storage. Storing the data in a lake enables the team building the pipeline to make data available to consumers without requiring them to structure all the data before obtaining value. Organizations frequently build their [data warehouse](https://en.wikipedia.org/wiki/Data_warehouse) and data marts on top of the lake, which is where consumers can access transformed data. The pipeline I will build in this example structures data, but stores it in a data lake simply because it's cheap and simple and it's not the focus of the post. The data lake will be on AWS S3, which is one of AWS' data lake [offerings](https://aws.amazon.com/big-data/datalakes-and-analytics/). 

Code & closing thoughts coming soon... 

